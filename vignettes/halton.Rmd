---
title: "Halton Sequences for Randomized Quasi-Monte Carlo (RQMC)"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{halton}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The original algorithm was written by [Art B. Owen](http://statweb.stanford.edu/~owen/) and can be found [here](http://statweb.stanford.edu/~owen/code/). I translated the internal function `randradinv`, which produces randomized (scrambled) van der Corput sequences into **C**, and it is where most of the computation is done.

The code snippet below shows how to use the `rhalton` function and is taken from Figure 5 of the paper found at the above link. We compare the standard error of the RQMC estimate to the CMC estimate; unsurprisingly RQMC is several orders of magnitude less variable.

```{r}
library(mcfun)

f <- function(v){sum(v)^2} # Example function that is easy for RMQC

# RQMC
R <- 10
n <- 5000
p <- 20
stride <- 1000 # Or replace 1000 by nthprime(0,getlength=TRUE)
muvec <- rep(0,R)
for(r in 1:R){
  x <- rhalton(n,p,singleseed = r*stride)
  muvec[r] <- mean( apply( x,1,f ) )
}
mu <- mean(muvec)
se <- sqrt(var(muvec)/R)

# CMC (crude Monte Carlo)
muvec_cmc <- rep(0,R)
for(r in 1:R){
  x <- matrix(data = runif(n*p),nrow = n,ncol = p)
  muvec_cmc[r] <- mean( apply( x,1,f ) )
}
mu_cmc <- mean(muvec_cmc)
se_cmc <- sqrt(var(muvec_cmc)/R)

knitr::kable(matrix(data=c(mu,se,mu_cmc,se_cmc),nrow=2,ncol=2,byrow=T,dimnames=list(c("RQMC","CMC"),c("mean","se"))),caption = "Comparison of RQMC to crude Monte Carlo on a simple problem")
```

Next we show RQMC's utility on a battery of more difficult problems. These are $d$-dimensional integrands of the form (where $x \in \mathbb{R}^{d}$):

$$ f_{k}(x) = g_{k}\left( \frac{1}{\sqrt{d}} \sum_{j=1}^{d} \Phi^{-1}(x_{j}) \right)$$

And specific functions $g_{k}$ are given as:

$$
  g_{1}(z) = \Phi(z+1) \\
  g_{2}(z) = \mathbb{1}_{z+1\geq 0} \\
  g_{3}(z) = \max(z+1,0) \\
  g_{4}(z) = \mathbb{1}_{z<\Phi^{-1}(0.001)}
$$

```{r}
# x: a d-dimensional vector
# g: a function
fk <- function(x,g){
  d <- length(x)
  z <- (1/sqrt(d))*sum(qnorm(x))
  g(z)
}

g1 <- function(z){
  pnorm(z+1)
}

g2 <- function(z){
  as.integer(z+1 >= 0)
}

g3 <- function(z){
  max(z+1,0)
}

g4 <- function(z){
  as.integer(z < qnorm(0.001))
}
```

We now compare the relative efficiency of RQMC versus MC on these problems across a range of dimensions $d$ and sample sizes $n$. First we numerically calculate $\mu_{k} = \mathbb{E}[g_{k}(z)]$ and $\sigma_{k}^{2}=\mathrm{Var}[g_{k}(z)]$ for each integrand which will be used in the theoretical estimate of crude Monte Carlo efficiency, $\frac{\sigma_{k}^{2}}{n}$.

```{r}
library(pracma)
mu1 <- integral(fun = Vectorize(fk,"x"),xmin=0,xmax=1,g=g1)
mu2 <- integral(fun = Vectorize(fk,"x"),xmin=0,xmax=1,g=g2)
mu3 <- integral(fun = Vectorize(fk,"x"),xmin=0,xmax=1,g=g3)
mu4 <- integral(fun = Vectorize(fk,"x"),xmin=0,xmax=1,g=g4)
```

Now we check RQMC's efficiency on $f_{1}$.


```{r}
n_sweep <- c(1e2,1e3,1e4,1e5)
d_sweep <- 1:50
R <- 100


```
